{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ae262d-a438-4327-a01f-73353d3aff60",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Model Serving Architecture</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9383c9-5948-453f-b5e3-50faf25e6ced",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Model Serving Architecture</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            De maneira geral, um Model Server hospedará o arquivo do modelo, assim como a API consumida pelo cliente.\n",
    "        </li>\n",
    "        <li>\n",
    "            As Big Techs desenvolvem softwares para serem utilizados em Model Servers.\n",
    "            <center style='margin-top:20px'>\n",
    "                <img src='img/04_02_server_providers.png'>\n",
    "            </center>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf90d4d-577c-4763-893d-39ec33d7356c",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Scaling Infrastructure</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e963b-795a-43d6-a9cf-be03d98199c4",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Scaling Infrastructure</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Escalonamento é uma técnica bastante comum no mundo de T.I, e não poderia ser diferente no de ML.\n",
    "        </li>\n",
    "        <li>\n",
    "            É recomendável haver algum mecanismo de escalonamento no ambiente de produção (preferencialmente horizontal), a fim de conseguirmos lidar com diferentes volumes de demanda, sem precisarmos indisponibilizar o nosso serviço.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b26caa-425c-49df-8c47-95761fd9f9f3",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Ferramentas de Escalabilidade</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Dada a enorme popularidade das ferramentas de containers, softwares como o Kubernetes e o Docker Swarm são bastante recorridas na criação de mecanismos de escalabilidade.\n",
    "        </li>\n",
    "        <li>\n",
    "            Com eles, o usuário poderá facilmente gerenciar o ciclo de vida de seus containers, assim como criar regras para a ativação/desligamento deles.\n",
    "        </li>\n",
    "        <li>\n",
    "            A equipe do Kubernetes desenvolveu o KubeFlow, software de MLOps para ser utilizado dentro de engines Kubernetes.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba02b-dc74-4d29-bc85-d217d696cb23",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Online Inference</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf0f11-f439-4cf4-baf7-e9173106e579",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Online Inference</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Tratando-se de inferências em tempo real, os três eixos de latência, throughput e custo deverão ser considerados para a arquitetação da infraestrutura de previsão.\n",
    "        </li>\n",
    "        <li>\n",
    "            Quando pensamos em oitimizar a latência de nossa IA, podemos considerar a melhora do servidor de deployment, busca por uma arquitetura de modelo mais rápida, ou, até mesmo, o aprimoramento da compilação do algoritmo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Caso a realização da inferência envolva a consulta em bases de dados para enriquecimento do $x$, recorrer a In-memory DB's pode ser prudente, ainda mais quando existem features frequentemente utilizadas.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dded07-5101-4a63-b81b-4c1ea12677ac",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Data Preprocessing</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712dd7f1-cfb7-4540-afdd-4b428b6b46e8",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Data Preprocessing</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A maior parte das inferências envolve uma camada de preprocessamento dos dados, para que a previsão seja concretizada.\n",
    "        </li>\n",
    "        <li>\n",
    "            É essencial que o fluxo de transformações esteja escrito da maneira mais eficiente possível, utilizando tecnologias de preprocessamento eficientes. Vale lembrar que usar in-memory DB's paraq possíveis JOIN's também é válido.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67a5b5-17a4-4e86-bb75-50d4139c03ec",
   "metadata": {},
   "source": [
    " <center style='margin-top:20px'> \n",
    "        <figure>\n",
    "            <img src='img/04_02_data_prep.png'>\n",
    "            <figcaption> Listagem de tipos de preprocessamento de dados.</figcaption>\n",
    "        </figure>\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a190a-47a1-4e7e-9437-ad4d7d67cbbb",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Batch Inference Scenarios</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873de44-9dfe-4273-b4fd-f8ff22be6956",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Batch Inference Scenarios</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            As previsões em batch possibilitam o uso de modelos mais complexos, sem a necessidade de baixíssima latência. No entanto, o throughput continua sendo uma variável importante na elaboração da infra de deployment.\n",
    "        </li>\n",
    "        <li>\n",
    "            Mas, observe, as features utilizadas no momento da inferência podem se tornar obsoletas no instante em que o usuário coletar as previsões.\n",
    "        </li>\n",
    "        <li>\n",
    "            Considere essa modalidade de previsão, quando não haver necessidade de tomadas de decisão rápidas.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339d7695-4a1f-41c9-af0a-e043f30cff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 56bcf6c] Online Inference (3:00)\n",
      " 2 files changed, 66 insertions(+), 15 deletions(-)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 1.27 KiB | 1.27 MiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/DeepLearningAI-MLOps.git\n",
      "   bbc3681..56bcf6c  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -am 'Fazer quiz Batch Inference Scenarios'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2328b-a1b0-40cd-9840-857e865403f2",
   "metadata": {},
   "source": [
    "<p style='color:red'> Vi Online Inference até Batch Inference Scenarios; Fazer quiz Batch Inference Scenarios\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
