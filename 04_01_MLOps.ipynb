{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3072dec-0b91-4215-9bbb-0760d71524c2",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'>  Introduction to Model Serving</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbf31c-ad36-4302-8e84-cc5e510ddf05",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Course Overview</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            O último curso da especialização é voltado ao ensinamento de técnicas e melhores práticas de implantações de modelos de ML.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8ccb5-3353-4277-9d05-d785f1a9dad5",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Introduction to Model Serving</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Ao se realizar o deploy de um algoritmo, os Engenheiros de ML devem se preocupar com a modalidade de inferência dos modelos (real-time ou batch), assim como o equilíbrio entre latência e throughput com custos. \n",
    "        </li>\n",
    "        <li>\n",
    "            No caso das questões de infra, costuma-se recorrer ao compartilhamento de GPU's e deploy multi-modelos, a fim de assegurarmos bons throughput e latência e economia de custos.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa807d5-4a13-49f9-9960-64199ca58d6b",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Introduction to Model Serving Infrastructure</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7b85a-1287-436f-80ab-82c31bdb18b1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Introduction to Model Serving Infrastructure</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A escolha do modelo ideal para implantação envolve muito mais do que somente a sua performance. O algoritmo também deve ser capaz de responder rapidamente suas requisições (real-time) e de caber na infraestrutura tida. \n",
    "        </li>\n",
    "        <li>\n",
    "            O instrutor sugere definirmos os critérios de infra e latência, antes da fase de modelagem. Assim, o Cientista será capaz de criar seus modelos orientado às restrições impostas, evitando surpresas no momento de deployment.\n",
    "        </li>\n",
    "        <li>\n",
    "            Outro ponto válido de se mencionar é que as requisições de nossos clientes podem não conter todos os dados necessários para a previsão do modelo. Nessa situação, é importante a existência de um fluxo de alta performance capaz de fazer os JOIN's necessários em um curto período de tempo.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168803a-ba1d-4514-8602-18ddf5cc0d7f",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Deployment Options</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Em situações em que há uma demanda por baixíssima latência (IoT), a implantação do modelo em edge device torna-se uma opção.\n",
    "        </li>\n",
    "        <li>\n",
    "            No entanto, lembre-se: dispositivos como celulares e veículos Não têm o mesmo poder computacional que servidores. Por isso, seu modelo deverá ter uma arquitetura menos complexa. Ainda assim, é preferível que a IA não consuma a totalidade dos recursos disponíveis, pois podem haver outros processos importantes que dependam deles. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe4d9-ff90-4733-88ad-4acb25ecd369",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style='font-size:40px'> Installing TensorFlow Serving</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b362e8-3ec4-4dea-a4b2-a5f7d2303902",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'>Installing TensorFlow Serving</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e34e3-f9c9-4d82-be15-3f77a108e5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master ff1953b] Deployment Options\n",
      " 1 file changed, 28 insertions(+), 7 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 959 bytes | 959.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/DeepLearningAI-MLOps.git\n",
      "   0752062..ff1953b  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -am 'Ver Installing TensorFlow Serving'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268c32f-babb-4ea7-ac18-24ba9bfa5820",
   "metadata": {},
   "source": [
    "<p style='color:red'> Iniciei curso 4; Ver Installing TensorFlow Serving\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
